{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code for testing the classifiers. Uses binary.csv and multiclass.csv\n",
    "\n",
    "Loads feature lists and tuning ranges from pickles in CWD.\n",
    "\n",
    "Change model types in models to tune on specific ones.\n",
    "\n",
    "Writes\n",
    "\n",
    "TODO: Add pickles once they are generated, more scoring metrics (maybe?), additional classifiers (maybe another flavour of SVM?)\n",
    "\n",
    "TODO: This hasn't been tested yet... :(\n",
    "\n",
    "Author: Wesley\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = pd.read_csv(\"binary.csv\")\n",
    "multiclass = pd.read_csv(\"multiclass.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (make labels numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text labels from the encoding will be passed to classification report so we can interpret our results more easily.\n",
      "\n",
      "Binary Label Encodings (in order of digits 0 -> 1): \n",
      "['BENIGN', 'ATTACK']\n",
      "\n",
      "Multiclass Label Encodings (in order of digits 0 -> n): \n",
      "['DNS', 'LDAP', 'MSSQL', 'NTP', 'NetBIOS', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDP', 'UDP-lag']\n"
     ]
    }
   ],
   "source": [
    "# Binarize binary dataset.\n",
    "binary[\" Label\"] = binary[\" Label\"].apply(lambda x: 0 if x == \"BENIGN\" else 1)\n",
    "\n",
    "print(\"The text labels from the encoding will be passed to classification report so we can interpret our results more easily.\\n\")\n",
    "binary_labels = [\"BENIGN\", \"ATTACK\"]\n",
    "\n",
    "print(\"Binary Label Encodings (in order of digits 0 -> 1): \")\n",
    "print(binary_labels)\n",
    "\n",
    "# Encode attack labels to int and save as array to be used later.\n",
    "le = LabelEncoder()\n",
    "multiclass[\" Label\"] = le.fit_transform(multiclass[\" Label\"].values)\n",
    "\n",
    "multiclass_labels = []\n",
    "print(\"\\nMulticlass Label Encodings (in order of digits 0 -> n): \")\n",
    "for i in range(0, len(list(set(list(multiclass[' Label']))))):\n",
    "    multiclass_labels.append(le.inverse_transform([i])[0])\n",
    "\n",
    "print(multiclass_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load feature sets and search spaces and enumerate their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = pickle.load(open(\"feature_sets.pickle\", 'rb'))\n",
    "search_spaces = pickle.load(open(\"hyperparameter_search_spaces.pickle\", 'rb'))\n",
    "\n",
    "print(f\"Available Tuning Ranges: {search_spaces.keys()}\")\n",
    "\n",
    "print(\"Feature Sets for Binary Dataset:\")\n",
    "for key, value in feature_sets[\"Binary\"].items():\n",
    "    if key == \"RFE Sets\":\n",
    "        print(value.keys())\n",
    "\n",
    "    elif key == \"PCA\":\n",
    "        print(f\"{key}, suggested variance threshold is {value}\")\n",
    "        \n",
    "    else:\n",
    "        print(key)\n",
    "\n",
    "print(\"Feature Sets for Multiclass Dataset:\")\n",
    "for key, value in feature_sets[\"Multiclass\"].items():\n",
    "    if key == \"RFE Sets\":\n",
    "        print(value.keys())\n",
    "\n",
    "    elif key == \"PCA\":\n",
    "        print(f\"{key}, suggested variance threshold is {value}\")\n",
    "\n",
    "    else:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a helper method to place our performance results in a DataFrame for future analysis.\n",
    "\"\"\"\n",
    "def format_results(y_test, predicted_values, fold_index, fitTime):\n",
    "    # get scores\n",
    "    accuracy = accuracy_score(y_test,predicted_values)\n",
    "    recall_pos = recall_score(y_test, predicted_values)\n",
    "    precision_pos = precision_score(y_test,predicted_values)\n",
    "    f1 = f1_score(y_test,predicted_values)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_values).ravel()\n",
    "\n",
    "    # This avoids divide by zero errors in some cases of no predicted samples.\n",
    "    if (tn + fp) > 0:\n",
    "        recall_neg = tn / (tn + fp)\n",
    "    else:\n",
    "        recall_neg = 0\n",
    "\n",
    "    if (tn + fn) > 0:\n",
    "        precision_neg = tn / (tn + fn)\n",
    "    else:\n",
    "        precision_neg = 0\n",
    "\n",
    "    cols = [\"Fitting Time\", \"accuracy\", \"TP\", \"TN\", \"FP\", \"FN\", \"Precision: 0\", \"Precision: 1\", \"Recall: 0\", \"Recall: 1\", \"F1 Score\"]\n",
    "    results = [fitTime, accuracy, tp, tn, fp, fn, precision_neg, precision_pos, recall_neg, recall_pos, f1]\n",
    "\n",
    "    outFrame = pd.DataFrame([results], columns=cols, index=[fold_index])\n",
    "\n",
    "    return outFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for tuning on the Binary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42),\n",
    "    \"Linear SVC\": make_pipeline(StandardScaler(), LinearSVC(random_state=42)),\n",
    "    \"Logistic Regression\": make_pipeline(StandardScaler, LogisticRegression(random_state=42)),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "score_methods = ['accuracy']\n",
    "\n",
    "feature_set = feature_sets[\"Binary\"]\n",
    "\n",
    "y = binary[\" Label\"].copy()\n",
    "X = binary.drop([\" Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "# This will hold all of our results.\n",
    "runFrame = None\n",
    "\n",
    "for name, model in models.items():\n",
    "        for feature_key, feature_val in feature_set.items():\n",
    "\n",
    "            # If we're on the RFE sets, check if we have one for this classifier. If not, skip it.\n",
    "            if feature_key == \"RFE Sets\":\n",
    "                if name in feature_val.keys():\n",
    "                    feature_val = feature_val[name]\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            for score_method in score_methods:\n",
    "                opt = BayesSearchCV(estimator=model,search_spaces=search_spaces[name],n_iter=50,scoring=score_method,cv=5,n_jobs=5)\n",
    "                kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "                counter = 0\n",
    "\n",
    "                # Used to hold data for a single run (performance metric)\n",
    "                perfFrame = None\n",
    "\n",
    "                for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    # PCA requires different logic to create X.\n",
    "                    if feature_key != \"PCA\":\n",
    "                        current_X = X.loc[:, feature_val]\n",
    "                        \n",
    "                    else:\n",
    "                        pca_trans = PCA(n_components=feature_val, random_state=42)\n",
    "                        X_pca = pca_trans.fit_transform(X)\n",
    "                        pca_cols = [\"PC\"+str(i) for i in list(range(1, len(X_pca[0])+1))]\n",
    "                        current_X = pd.DataFrame(data=X_pca, columns=pca_cols)\n",
    "\n",
    "                    X_train, X_test = current_X.iloc[train_index,:], current_X.iloc[test_index,:]\n",
    "                    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    startTime = time()\n",
    "\n",
    "                    opt.fit(X_train,Y_train)\n",
    "\n",
    "                    endTime = time()\n",
    "                    fitTime = endTime - startTime\n",
    "\n",
    "                    predicted_values = opt.predict(X_test)\n",
    "\n",
    "                    # get metrics for this fold.\n",
    "                    foldFrame = format_results(Y_test, predicted_values, counter, fitTime)\n",
    "\n",
    "                    # Add them to our lists of metric.\n",
    "                    if perfFrame is None:\n",
    "                        perfFrame = foldFrame\n",
    "                    else:\n",
    "                        perfFrame = pd.concat([perfFrame, foldFrame])\n",
    "\n",
    "                    # Print a classification report as well.\n",
    "                    print(classification_report(Y_test, predicted_values, target_names=binary_labels))\n",
    "\n",
    "                    # Add tuple with the best params as well as the related model/config\n",
    "                    params.append((f\"Binary {name} {feature_key} {score_method} Fold {counter}\", opt.best_params_))\n",
    "\n",
    "                # Create a new line in the results table that averages all the folds\n",
    "                perfFrame.loc[\"fold average\"] = perfFrame.mean()\n",
    "\n",
    "                # Mark the results table with the chosen classifier and the current performance metric.\n",
    "                perfFrame['metric'] = [score_method for j in range(0,6)]\n",
    "                perfFrame['Classifier'] = [name for j in range(0,6)]\n",
    "                perfFrame['Feature Set'] = [feature_key for j in range(0,6)]\n",
    "                perfFrame['Dataset'] = [\"binary\" for j in range(0,6)]\n",
    "                print(f\"{name} with {feature_key} and {score_method} completed.\")\n",
    "\n",
    "                # Add this run to the table with all runs.\n",
    "                if runFrame is None:\n",
    "                    runFrame = perfFrame\n",
    "                else:\n",
    "                    runFrame = pd.concat([runFrame, perfFrame])\n",
    "\n",
    "# Write output file\n",
    "runFrame.to_csv(f\"binary_results{time()}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for tuning on the multiclass set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42, num_classes=11, objective='multi:softmax'),\n",
    "    \"Linear SVC\": make_pipeline(StandardScaler(), LinearSVC(random_state=42)),\n",
    "    \"Logistic Regression\": make_pipeline(StandardScaler, LogisticRegression(random_state=42)),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "score_methods = ['accuracy']\n",
    "\n",
    "feature_set = feature_sets[\"Multiclass\"]\n",
    "\n",
    "y = multiclass[\" Label\"].copy()\n",
    "X = multiclass.drop([\" Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "# This will hold all of our results.\n",
    "runFrame = None\n",
    "\n",
    "for name, model in models.items():\n",
    "        for feature_key, feature_val in feature_set.items():\n",
    "\n",
    "            # If we're on the RFE sets, check if we have one for this classifier. If not, skip it.\n",
    "            if feature_key == \"RFE Sets\":\n",
    "                if name in feature_val.keys():\n",
    "                    feature_val = feature_val[name]\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            for score_method in score_methods:\n",
    "                opt = BayesSearchCV(estimator=model,search_spaces=search_spaces[name],n_iter=50,scoring=score_method,cv=5,n_jobs=5)\n",
    "                kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "                counter = 0\n",
    "\n",
    "                # Used to hold data for a single run (performance metric)\n",
    "                perfFrame = None\n",
    "\n",
    "                for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    # PCA requires different logic to create X.\n",
    "                    if feature_key != \"PCA\":\n",
    "                        current_X = X.loc[:, feature_val]\n",
    "                        \n",
    "                    else:\n",
    "                        pca_trans = PCA(n_components=feature_val, random_state=42)\n",
    "                        X_pca = pca_trans.fit_transform(X)\n",
    "                        pca_cols = [\"PC\"+str(i) for i in list(range(1, len(X_pca[0])+1))]\n",
    "                        current_X = pd.DataFrame(data=X_pca, columns=pca_cols)\n",
    "\n",
    "                    X_train, X_test = current_X.iloc[train_index,:], current_X.iloc[test_index,:]\n",
    "                    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    startTime = time()\n",
    "\n",
    "                    opt.fit(X_train,Y_train)\n",
    "\n",
    "                    endTime = time()\n",
    "                    fitTime = endTime - startTime\n",
    "\n",
    "                    predicted_values = opt.predict(X_test)\n",
    "\n",
    "                    # get metrics for this fold.\n",
    "                    foldFrame = format_results(Y_test, predicted_values, counter, fitTime)\n",
    "\n",
    "                    # Add them to our lists of metric.\n",
    "                    if perfFrame is None:\n",
    "                        perfFrame = foldFrame\n",
    "                    else:\n",
    "                        perfFrame = pd.concat([perfFrame, foldFrame])\n",
    "\n",
    "                    # Print a classification report as well.\n",
    "                    print(classification_report(Y_test, predicted_values, target_names=multiclass_labels))\n",
    "\n",
    "                    # Add tuple with the best params as well as the related model/config\n",
    "                    params.append((f\"Binary {name} {feature_key} {score_method} Fold {counter}\", opt.best_params_))\n",
    "\n",
    "                # Create a new line in the results table that averages all the folds\n",
    "                perfFrame.loc[\"fold average\"] = perfFrame.mean()\n",
    "\n",
    "                # Mark the results table with the chosen classifier and the current performance metric.\n",
    "                perfFrame['metric'] = [score_method for j in range(0,6)]\n",
    "                perfFrame['Classifier'] = [name for j in range(0,6)]\n",
    "                perfFrame['Feature Set'] = [feature_key for j in range(0,6)]\n",
    "                perfFrame['Dataset'] = [\"multiclass\" for j in range(0,6)]\n",
    "                print(f\"{name} with {feature_key} and {score_method} completed.\")\n",
    "\n",
    "                # Add this run to the table with all runs.\n",
    "                if runFrame is None:\n",
    "                    runFrame = perfFrame\n",
    "                else:\n",
    "                    runFrame = pd.concat([runFrame, perfFrame])\n",
    "\n",
    "# Write output file\n",
    "runFrame.to_csv(f\"multiclass_results{time()}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4593b52a31704b5155580280f0f05a500adb36bf97584c7e94e26dfe17ea16d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
