{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a custom transformer that allows us to reduce the feature sets for each classifier appropriately.\n",
    "\n",
    "Necessary since we're making them all part of a StackingClassifier and each one uses a different feature set.\n",
    "\"\"\"\n",
    "class FeatureReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_list=None):\n",
    "        self.feature_list = feature_list\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        if self.feature_list is None:\n",
    "            return X\n",
    "        \n",
    "        else:\n",
    "            return X.loc[:, self.feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiclass Label Encodings (in order of digits 0 -> n): \n",
      "['DNS', 'LDAP', 'MSSQL', 'NTP', 'NetBIOS', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDP', 'UDP-lag']\n"
     ]
    }
   ],
   "source": [
    "multiclass_train = pd.read_csv(\"../multiclass_train.csv\")\n",
    "multiclass_test = pd.read_csv(\"../multiclass_test.csv\")\n",
    "binary_train = pd.read_csv(\"../binary_train.csv\")\n",
    "binary_test = pd.read_csv(\"../binary_test.csv\")\n",
    "\n",
    "multi_train_y = multiclass_train[\" Label\"].copy()\n",
    "multi_train_x = multiclass_train.drop([\" Label\"], axis = 1)\n",
    "\n",
    "multi_test_y = multiclass_test[\" Label\"].copy()\n",
    "multi_test_x = multiclass_test.drop([\" Label\"], axis = 1)\n",
    "\n",
    "bin_train_y = binary_train[\" Label\"].copy()\n",
    "bin_train_x = binary_train.drop([\" Label\"], axis = 1)\n",
    "\n",
    "bin_test_y = binary_test[\" Label\"].copy()\n",
    "bin_test_x = binary_test.drop([\" Label\"], axis = 1)\n",
    "\n",
    "# Encoding\n",
    "bin_train_y = [0 if x==\"BENIGN\" else 1 for x in bin_train_y.values]\n",
    "bin_test_y = [0 if x==\"BENIGN\" else 1 for x in bin_test_y.values]\n",
    "\n",
    "# Encode attack labels to int and save as array to be used later.\n",
    "le = LabelEncoder()\n",
    "\n",
    "multi_y_train = le.fit_transform(multi_train_y.values)\n",
    "multi_y_test = le.transform(multi_test_y.values)\n",
    "\n",
    "multiclass_labels = []\n",
    "print(\"\\nMulticlass Label Encodings (in order of digits 0 -> n): \")\n",
    "for i in range(0, len(list(set(list(multi_y_test))))):\n",
    "    multiclass_labels.append(le.inverse_transform([i])[0])\n",
    "\n",
    "print(multiclass_labels)\n",
    "\n",
    "binary_labels = [\"BENIGN\", \"ATTACK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_params_DT.pickle\n",
      "Binary Decision Tree All accuracy Fold 3, 0.998786996595658: OrderedDict([('criterion', 'entropy'), ('max_depth', 25), ('max_features', 'sqrt'), ('min_samples_leaf', 1), ('min_samples_split', 2)])\n",
      "\n",
      "\n",
      "binary_params_KNN.pickle\n",
      "Binary KNN Mutual Information accuracy Fold 1, 0.9914011334012937: OrderedDict([('algorithm', 'ball_tree'), ('n_neighbors', 4), ('weights', 'distance')])\n",
      "\n",
      "\n",
      "binary_params_LR.pickle\n",
      "Binary Logistic Regression All accuracy Fold 3, 0.9932879788100721: OrderedDict([('logisticregression__C', 0.2568671912905853), ('logisticregression__max_iter', 500)])\n",
      "\n",
      "\n",
      "binary_params_NB.pickle\n",
      "Binary Naive Bayes All accuracy Fold 3, 0.6620260531578819: OrderedDict([('var_smoothing', 1e-09)])\n",
      "\n",
      "\n",
      "binary_params_RF.pickle\n",
      "Binary Random Forest All accuracy Fold 2, 0.999460887302596: OrderedDict([('criterion', 'entropy'), ('max_depth', 100), ('max_features', 'sqrt'), ('min_samples_leaf', 1), ('min_samples_split', 7), ('n_estimators', 344)])\n",
      "\n",
      "\n",
      "multiclass_params_DT.pickle\n",
      "Multiclass Decision Tree RFE Sets accuracy Fold 4, 0.7947435527405508: OrderedDict([('criterion', 'entropy'), ('max_depth', 18), ('max_features', 'sqrt'), ('min_samples_leaf', 1), ('min_samples_split', 10)])\n",
      "\n",
      "\n",
      "multiclass_params_KNN.pickle\n",
      "Multiclass KNN Mutual Information accuracy Fold 3, 0.7112160260966905: OrderedDict([('algorithm', 'ball_tree'), ('n_neighbors', 7), ('weights', 'distance')])\n",
      "\n",
      "\n",
      "multiclass_params_LR.pickle\n",
      "Multiclass Logistic Regression RFE Sets accuracy Fold 2, 0.6589460779553828: OrderedDict([('logisticregression__C', 9972.476141278883), ('logisticregression__max_iter', 483)])\n",
      "\n",
      "\n",
      "multiclass_params_NB.pickle\n",
      "Multiclass Naive Bayes Correlation accuracy Fold 3, 0.25706589840691485: OrderedDict([('var_smoothing', 9.99835896078805e-08)])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileList = [os.getcwd() + '/' + f for f in listdir(os.getcwd()) if (isfile(join(os.getcwd(), f)) and \"pickle\" in f)]\n",
    "\n",
    "for f in fileList:\n",
    "    targetIndex = f.rfind(\"/\") + 1\n",
    "    fileName = f[targetIndex:len(f)]\n",
    "\n",
    "    print(fileName)\n",
    "\n",
    "    params = pickle.load(open(f, 'rb'))\n",
    "\n",
    "    score_list = []\n",
    "    for x in params:\n",
    "        score_list.append(x[1])\n",
    "\n",
    "    ind = np.argmax(score_list)\n",
    "        \n",
    "    print(f\"{params[ind][0]}, {params[ind][1]}: {params[ind][2]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = pickle.load(open(\"../feature_sets.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_bin = make_pipeline(FeatureReducer(), DecisionTreeClassifier(random_state = 42, criterion = 'entropy', max_depth = 25, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 2))\n",
    "dt_mult = make_pipeline(FeatureReducer(feature_list = feature_sets[\"Multiclass\"][\"RFE Sets\"][\"Decision Tree\"]), DecisionTreeClassifier(random_state = 42, criterion = 'entropy', max_depth = 18, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 10))\n",
    "\n",
    "knn_bin = make_pipeline(FeatureReducer(feature_list = feature_sets[\"Binary\"][\"Mutual Information\"]), KNeighborsClassifier(algorithm = 'ball_tree', n_neighbors =  4, weights = 'distance'))\n",
    "knn_mult = make_pipeline(FeatureReducer(feature_list = feature_sets[\"Multiclass\"][\"Mutual Information\"]), KNeighborsClassifier(algorithm = 'ball_tree', n_neighbors = 7, weights =  'distance'))\n",
    "\n",
    "lr_bin = make_pipeline(FeatureReducer(), StandardScaler(), LogisticRegression(C = 0.2568671912905853, max_iter = 500, random_state=42))\n",
    "lr_mult = make_pipeline(FeatureReducer(feature_list = feature_sets[\"Multiclass\"][\"RFE Sets\"][\"Logistic Regression\"]), StandardScaler(), LogisticRegression(C = 9972.476141278883, max_iter =  483, random_state=42))\n",
    "\n",
    "nb_bin = make_pipeline(FeatureReducer(), GaussianNB(var_smoothing = 1e-09))\n",
    "nb_mult = make_pipeline(FeatureReducer(feature_list = feature_sets[\"Multiclass\"][\"Correlation\"]), GaussianNB(var_smoothing = 9.99835896078805e-08))\n",
    "\n",
    "rf_bin = make_pipeline(FeatureReducer(), RandomForestClassifier(random_state=42, criterion = 'entropy', max_depth = 100, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 7, n_estimators = 344))\n",
    "rf_mult = make_pipeline(FeatureReducer(), RandomForestClassifier(random_state=42, criterion = 'entropy', max_depth = 100, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 7, n_estimators = 344))\n",
    "\n",
    "binary_classifiers = [(dt_bin, \"Decision Tree\"), (knn_bin, \"KNN\"), (lr_bin, \"Logistic Regression\"), (nb_bin, \"Naive Bayes\"), (rf_bin, \"Random Forest\")]\n",
    "multiclass_classifiers = [(dt_mult, \"Decision Tree\"), (knn_mult, \"KNN\"), (lr_mult, \"Logistic Regression\"), (nb_mult, \"Naive Bayes\"), (rf_mult, \"Random Forest\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classifiers:\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN   0.998167  0.998329  0.998248     18548\n",
      "      ATTACK   0.998329  0.998167  0.998248     18550\n",
      "\n",
      "    accuracy                       0.998248     37098\n",
      "   macro avg   0.998248  0.998248  0.998248     37098\n",
      "weighted avg   0.998248  0.998248  0.998248     37098\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN   0.993135  0.990565  0.991848     18548\n",
      "      ATTACK   0.990590  0.993154  0.991870     18550\n",
      "\n",
      "    accuracy                       0.991859     37098\n",
      "   macro avg   0.991863  0.991859  0.991859     37098\n",
      "weighted avg   0.991863  0.991859  0.991859     37098\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN   0.989930  0.996442  0.993175     18548\n",
      "      ATTACK   0.996418  0.989865  0.993131     18550\n",
      "\n",
      "    accuracy                       0.993153     37098\n",
      "   macro avg   0.993174  0.993153  0.993153     37098\n",
      "weighted avg   0.993175  0.993153  0.993153     37098\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN   0.609943  0.939940  0.739810     18548\n",
      "      ATTACK   0.869172  0.398976  0.546906     18550\n",
      "\n",
      "    accuracy                       0.669443     37098\n",
      "   macro avg   0.739558  0.669458  0.643358     37098\n",
      "weighted avg   0.739564  0.669443  0.643353     37098\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN   0.999030  0.999892  0.999461     18548\n",
      "      ATTACK   0.999892  0.999030  0.999461     18550\n",
      "\n",
      "    accuracy                       0.999461     37098\n",
      "   macro avg   0.999461  0.999461  0.999461     37098\n",
      "weighted avg   0.999461  0.999461  0.999461     37098\n",
      "\n",
      "Multiclass Classifiers:\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DNS   0.666424  0.591394  0.626671      3091\n",
      "        LDAP   0.698213  0.770948  0.732780      3091\n",
      "       MSSQL   0.816661  0.789712  0.802961      3091\n",
      "         NTP   0.992225  0.990621  0.991423      3092\n",
      "     NetBIOS   0.759664  0.730918  0.745014      3092\n",
      "     Portmap   0.861386  0.956972  0.906667      3091\n",
      "        SNMP   0.778354  0.776843  0.777598      3092\n",
      "        SSDP   0.543884  0.621281  0.580012      3092\n",
      "         Syn   0.993056  0.971530  0.982175      3091\n",
      "        TFTP   0.999350  0.995147  0.997244      3091\n",
      "         UDP   0.540170  0.472016  0.503798      3091\n",
      "     UDP-lag   0.860956  0.845084  0.852946      3092\n",
      "\n",
      "    accuracy                       0.792706     37097\n",
      "   macro avg   0.792529  0.792706  0.791607     37097\n",
      "weighted avg   0.792528  0.792706  0.791607     37097\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DNS   0.560507  0.472016  0.512469      3091\n",
      "        LDAP   0.653143  0.682303  0.667405      3091\n",
      "       MSSQL   0.671111  0.732773  0.700588      3091\n",
      "         NTP   0.854097  0.906856  0.879686      3092\n",
      "     NetBIOS   0.690265  0.580207  0.630469      3092\n",
      "     Portmap   0.831137  1.000000  0.907783      3091\n",
      "        SNMP   0.756579  0.706662  0.730769      3092\n",
      "        SSDP   0.468193  0.476067  0.472097      3092\n",
      "         Syn   0.898708  0.855387  0.876513      3091\n",
      "        TFTP   0.984224  0.989000  0.986606      3091\n",
      "         UDP   0.477583  0.485927  0.481719      3091\n",
      "     UDP-lag   0.733681  0.727038  0.730344      3092\n",
      "\n",
      "    accuracy                       0.717848     37097\n",
      "   macro avg   0.714936  0.717853  0.714704     37097\n",
      "weighted avg   0.714934  0.717848  0.714701     37097\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DNS   0.364865  0.104820  0.162855      3091\n",
      "        LDAP   0.563423  0.715626  0.630469      3091\n",
      "       MSSQL   0.552775  0.850534  0.670065      3091\n",
      "         NTP   0.898577  0.959897  0.928225      3092\n",
      "     NetBIOS   0.798246  0.382600  0.517272      3092\n",
      "     Portmap   0.587833  0.806535  0.680033      3091\n",
      "        SNMP   0.652872  0.683700  0.667930      3092\n",
      "        SSDP   0.447510  0.671410  0.537059      3092\n",
      "         Syn   0.985244  0.928826  0.956203      3091\n",
      "        TFTP   0.987055  0.986736  0.986895      3091\n",
      "         UDP   0.434381  0.304109  0.357755      3091\n",
      "     UDP-lag   0.905945  0.719599  0.802091      3092\n",
      "\n",
      "    accuracy                       0.676200     37097\n",
      "   macro avg   0.681560  0.676199  0.658071     37097\n",
      "weighted avg   0.681568  0.676200  0.658075     37097\n",
      "\n",
      "Naive Bayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DNS   0.159827  0.023940  0.041643      3091\n",
      "        LDAP   0.124065  0.064380  0.084771      3091\n",
      "       MSSQL   0.000000  0.000000  0.000000      3091\n",
      "         NTP   0.000000  0.000000  0.000000      3092\n",
      "     NetBIOS   0.949367  0.242561  0.386399      3092\n",
      "     Portmap   1.000000  0.002265  0.004519      3091\n",
      "        SNMP   0.330399  0.655563  0.439363      3092\n",
      "        SSDP   0.061321  0.033635  0.043442      3092\n",
      "         Syn   0.540964  0.892915  0.673746      3091\n",
      "        TFTP   0.891704  0.956325  0.922885      3091\n",
      "         UDP   0.162024  0.942737  0.276523      3091\n",
      "     UDP-lag   0.000000  0.000000  0.000000      3092\n",
      "\n",
      "    accuracy                       0.317842     37097\n",
      "   macro avg   0.351639  0.317860  0.239441     37097\n",
      "weighted avg   0.351628  0.317842  0.239432     37097\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DNS   0.729989  0.616629  0.668537      3091\n",
      "        LDAP   0.724824  0.801035  0.761027      3091\n",
      "       MSSQL   0.818777  0.849240  0.833730      3091\n",
      "         NTP   0.994167  0.992238  0.993202      3092\n",
      "     NetBIOS   0.823031  0.753558  0.786763      3092\n",
      "     Portmap   0.897764  1.000000  0.946128      3091\n",
      "        SNMP   0.805136  0.811125  0.808120      3092\n",
      "        SSDP   0.552126  0.566947  0.559438      3092\n",
      "         Syn   0.986877  0.973148  0.979964      3091\n",
      "        TFTP   1.000000  0.994824  0.997405      3091\n",
      "         UDP   0.541346  0.546425  0.543874      3091\n",
      "     UDP-lag   0.869404  0.835382  0.852053      3092\n",
      "\n",
      "    accuracy                       0.811710     37097\n",
      "   macro avg   0.811953  0.811713  0.810854     37097\n",
      "weighted avg   0.811953  0.811710  0.810852     37097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Classifiers:\")\n",
    "for x in binary_classifiers:\n",
    "    print(x[1])\n",
    "    model = x[0]\n",
    "\n",
    "    model.fit(bin_train_x, bin_train_y)\n",
    "\n",
    "    y_pred = model.predict(bin_test_x)\n",
    "\n",
    "    print(classification_report(bin_test_y, y_pred, digits = 6, target_names = binary_labels))\n",
    "\n",
    "print(\"Multiclass Classifiers:\")\n",
    "for x in multiclass_classifiers:\n",
    "    print(x[1])\n",
    "    model = x[0]\n",
    "\n",
    "    model.fit(multi_train_x, multi_train_y)\n",
    "\n",
    "    y_pred = model.predict(multi_test_x)\n",
    "\n",
    "    print(classification_report(multi_test_y, y_pred, digits = 6, target_names = multiclass_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4593b52a31704b5155580280f0f05a500adb36bf97584c7e94e26dfe17ea16d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
